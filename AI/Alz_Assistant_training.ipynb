{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'visualkeras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d833b4ad995f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mvisualkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'visualkeras'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import tensorflow\n",
    "import tflearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = json.loads(open('intents.json').read())\n",
    "\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words= []\n",
    "classes=[]\n",
    "documents=[]\n",
    "ignore_letters = ['?','!','.',',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list,intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents), \"documents\")\n",
    "print(len(classes), \"classes\", classes)\n",
    "print(len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output=[]\n",
    "output_empty = [0]* len(classes)\n",
    "\n",
    "for x,doc in enumerate(documents):\n",
    "    bag = []\n",
    "    word_patterns= doc[0]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    \n",
    "    output_row = output_empty[:]\n",
    "    index = classes.index(doc[1])\n",
    "    output_row[index] =1\n",
    "    training.append([bag,output_row])\n",
    "    \n",
    "    \n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcheck(AI_word,inplst):\n",
    "    checklst=[]\n",
    "    for i in range(len(inplst)):\n",
    "        if inplst[i].lower()==AI_word.lower():\n",
    "            checklst.append(True)\n",
    "        else:\n",
    "            checklst.append(False)\n",
    "    return any(checklst)\n",
    "def synonymCheck(AI_word,inplst):\n",
    "    synlst=[]\n",
    "    checklst=[]\n",
    "    for vsyn in wordnet.synsets(AI_word):\n",
    "        for l in vsyn.lemmas():\n",
    "            synlst.append(l.name())\n",
    "    for i in range(len(inplst)):\n",
    "        for j in range(len(synlst)):\n",
    "            if (inplst[i].lower()==synlst[j].lower()):\n",
    "                checklst.append(True)\n",
    "            else:\n",
    "                checklst.append(False)\n",
    "    return any(checklst)\n",
    "\n",
    "\n",
    "def inputter(AIList):\n",
    "    outlst=[] #out list of length len(AIList)\n",
    "    inputstr=input(\"Ask a question:\")\n",
    "    inputlst1=nltk.word_tokenize(inputstr)\n",
    "    inputlst=[lemmatizer.lemmatize(word.lower()) for word in inputlst1]\n",
    "    for AIwordIndex in range(len(AIList)):\n",
    "        if (wordcheck(AIList[AIwordIndex],inputlst)==True) or (synonymCheck(AIList[AIwordIndex],inputlst)==True):\n",
    "            outlst.append(1)\n",
    "        else:\n",
    "            outlst.append(0)\n",
    "    return outlst\n",
    "\n",
    "ai_data = inputter(words)\n",
    "ai_data_np = np.array([ai_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = model.predict(ai_data_np)[0]\n",
    "results = [[i,r] for i,r in enumerate(results)]\n",
    "# sort by strength of probability\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "return_list = []\n",
    "for r in results:\n",
    "    return_list.append((classes[r[0]], str(r[1])))\n",
    "    # return tuple of intent and probability\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=classes[results[0][0]]\n",
    "print(tag)\n",
    "tag_index= tag.split(\"_\")\n",
    "tag_index=int(tag_index[0])\n",
    "responses = intents['intents'][tag_index-1]['responses']\n",
    "print(random.choice(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
